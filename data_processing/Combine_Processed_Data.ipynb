{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b2770c",
   "metadata": {},
   "source": [
    "# Combine Processed Sensor Data\n",
    "\n",
    "This notebook combines all the processed sensor datasets into a unified dataset for federated learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e220ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df44d08d",
   "metadata": {},
   "source": [
    "## Load All Processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c1b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_datasets(base_path):\n",
    "    \"\"\"Load all processed sensor datasets\"\"\"\n",
    "    sensors = [\n",
    "        'Fridge',\n",
    "        'Garage_Door',\n",
    "        'GPS_Tracker',\n",
    "        'Modbus',\n",
    "        'Motion_Light',\n",
    "        'Thermostat',\n",
    "        'Weather'\n",
    "    ]\n",
    "    \n",
    "    datasets = {}\n",
    "    for sensor in sensors:\n",
    "        file_path = os.path.join(base_path, f'Processed_{sensor.lower()}_sensor_dataset.csv')\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                datasets[sensor] = df\n",
    "                print(f\"Loaded {sensor} dataset with shape {df.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {sensor} dataset: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"Processed dataset not found for {sensor}\")\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce4a61",
   "metadata": {},
   "source": [
    "## Analyze Dataset Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_features(datasets):\n",
    "    \"\"\"Analyze features across all datasets\"\"\"\n",
    "    feature_analysis = {}\n",
    "    \n",
    "    for sensor, df in datasets.items():\n",
    "        feature_analysis[sensor] = {\n",
    "            'columns': list(df.columns),\n",
    "            'dtypes': df.dtypes.to_dict(),\n",
    "            'num_features': len(df.columns),\n",
    "            'num_samples': len(df)\n",
    "        }\n",
    "    \n",
    "    return feature_analysis\n",
    "\n",
    "def print_feature_analysis(feature_analysis):\n",
    "    \"\"\"Print feature analysis in a readable format\"\"\"\n",
    "    for sensor, analysis in feature_analysis.items():\n",
    "        print(f\"\\n{sensor} Dataset:\")\n",
    "        print(f\"Number of samples: {analysis['num_samples']}\")\n",
    "        print(f\"Number of features: {analysis['num_features']}\")\n",
    "        print(\"Features:\")\n",
    "        for col, dtype in analysis['dtypes'].items():\n",
    "            print(f\"  - {col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8175f7fa",
   "metadata": {},
   "source": [
    "## Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820470d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_datasets(datasets, feature_analysis):\n",
    "    \"\"\"Combine all datasets with proper feature alignment\"\"\"\n",
    "    combined_data = []\n",
    "    common_features = None\n",
    "    \n",
    "    # Find common features across all datasets\n",
    "    for sensor, analysis in feature_analysis.items():\n",
    "        if common_features is None:\n",
    "            common_features = set(analysis['columns'])\n",
    "        else:\n",
    "            common_features = common_features.intersection(set(analysis['columns']))\n",
    "    \n",
    "    common_features = list(common_features)\n",
    "    print(f\"Found {len(common_features)} common features across all datasets:\")\n",
    "    print(common_features)\n",
    "    \n",
    "    # Combine datasets using common features\n",
    "    for sensor, df in datasets.items():\n",
    "        # Add source identifier\n",
    "        df_subset = df[common_features].copy()\n",
    "        df_subset['source_sensor'] = sensor\n",
    "        combined_data.append(df_subset)\n",
    "    \n",
    "    combined_df = pd.concat(combined_data, axis=0, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78133dbd",
   "metadata": {},
   "source": [
    "## Generate Combined Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ad1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_combined_dataset(combined_df):\n",
    "    \"\"\"Generate analysis and visualizations for the combined dataset\"\"\"\n",
    "    print(\"\\nCombined Dataset Analysis:\")\n",
    "    print(f\"Total samples: {len(combined_df)}\")\n",
    "    print(f\"Total features: {len(combined_df.columns)}\")\n",
    "    print(\"\\nSamples per sensor:\")\n",
    "    print(combined_df['source_sensor'].value_counts())\n",
    "    \n",
    "    # Generate correlation heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    numeric_df = combined_df.select_dtypes(include=[np.number])\n",
    "    sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')\n",
    "    plt.title('Correlation Heatmap - Combined Dataset')\n",
    "    plt.savefig('Correlation_heatmap_combined.png', dpi=600, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa847ca",
   "metadata": {},
   "source": [
    "## Run the Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'path/to/processed/data'\n",
    " \n",
    "# Load all processed datasets\n",
    "datasets = load_processed_datasets(base_path)\n",
    " \n",
    "# Analyze features\n",
    "feature_analysis = analyze_features(datasets)\n",
    "print_feature_analysis(feature_analysis)\n",
    " \n",
    "# Combine datasets\n",
    "combined_df = combine_datasets(datasets, feature_analysis)\n",
    " \n",
    "# Analyze combined dataset\n",
    "combined_df = analyze_combined_dataset(combined_df)\n",
    " \n",
    "# Save combined dataset\n",
    "combined_df.to_csv('combined_sensor_dataset.csv', index=False)\n",
    "print(\"\\nCombined dataset saved to 'combined_sensor_dataset.csv'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
