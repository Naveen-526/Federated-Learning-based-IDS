{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be2fb990",
   "metadata": {},
   "source": [
    "# Global Data Processing for All Sensors\n",
    "\n",
    "This notebook provides a unified preprocessing pipeline for all sensor datasets. It can process any of the sensor datasets (Fridge, Garage Door, GPS Tracker, Modbus, Motion Light, Thermostat, and Weather) using a consistent approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import plotly.express as px\n",
    "import os\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b049dd",
   "metadata": {},
   "source": [
    "## Define Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9572f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    \"\"\"Load a dataset from the given path\"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"Handle missing values in the dataset\"\"\"\n",
    "    # Fill numeric columns with mean\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())\n",
    "    \n",
    "    # Fill categorical columns with mode\n",
    "    categorical_columns = df.select_dtypes(exclude=[np.number]).columns\n",
    "    df[categorical_columns] = df[categorical_columns].fillna(df[categorical_columns].mode().iloc[0])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def encode_categorical_features(df):\n",
    "    \"\"\"Encode categorical features using LabelEncoder\"\"\"\n",
    "    label_encoders = {}\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for column in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        label_encoders[column] = le\n",
    "    \n",
    "    return df, label_encoders\n",
    "\n",
    "def scale_features(df):\n",
    "    \"\"\"Scale numerical features using MinMaxScaler\"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "    return df, scaler\n",
    "\n",
    "def handle_imbalanced_data(X, y):\n",
    "    \"\"\"Handle imbalanced data using RandomOverSampler\"\"\"\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def generate_visualizations(df, sensor_name):\n",
    "    \"\"\"Generate and save visualizations for the dataset\"\"\"\n",
    "    # Correlation heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "    plt.title(f'Correlation Heatmap - {sensor_name}')\n",
    "    plt.savefig(f'Correlation_heatmap_{sensor_name.lower()}.png', dpi=600, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Pairplot\n",
    "    sns.pairplot(df)\n",
    "    plt.savefig(f'Pairplot_{sensor_name.lower()}.png', dpi=600)\n",
    "    plt.close()\n",
    "\n",
    "def process_dataset(file_path, sensor_name, target_column=None):\n",
    "    \"\"\"Process a single dataset with the complete pipeline\"\"\"\n",
    "    # Load the dataset\n",
    "    print(f\"Processing {sensor_name} dataset...\")\n",
    "    df = load_dataset(file_path)\n",
    "    \n",
    "    # Handle missing values\n",
    "    df = handle_missing_values(df)\n",
    "    \n",
    "    # Encode categorical features\n",
    "    df, label_encoders = encode_categorical_features(df)\n",
    "    \n",
    "    # Scale features\n",
    "    df, scaler = scale_features(df)\n",
    "    \n",
    "    # Handle imbalanced data if target column is provided\n",
    "    if target_column and target_column in df.columns:\n",
    "        y = df[target_column]\n",
    "        X = df.drop(target_column, axis=1)\n",
    "        X_resampled, y_resampled = handle_imbalanced_data(X, y)\n",
    "        df = pd.concat([pd.DataFrame(X_resampled, columns=X.columns),\n",
    "                       pd.Series(y_resampled, name=target_column)], axis=1)\n",
    "    \n",
    "    # Generate visualizations\n",
    "    generate_visualizations(df, sensor_name)\n",
    "    \n",
    "    # Save processed dataset\n",
    "    output_file = f'Processed_{sensor_name.lower()}_sensor_dataset.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Processed data saved to {output_file}\")\n",
    "    \n",
    "    return df, label_encoders, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2903197d",
   "metadata": {},
   "source": [
    "## Process All Datasets\n",
    "\n",
    "To process a dataset, call the `process_dataset` function with the appropriate parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Replace 'path_to_dataset.csv' with the actual path to your sensor dataset\n",
    "# Replace 'target_column_name' with the actual target column name if applicable\n",
    "\n",
    "def process_all_sensors(base_path):\n",
    "    \"\"\"Process all sensor datasets in the given directory\"\"\"\n",
    "    sensors = [\n",
    "        'Fridge',\n",
    "        'Garage_Door',\n",
    "        'GPS_Tracker',\n",
    "        'Modbus',\n",
    "        'Motion_Light',\n",
    "        'Thermostat',\n",
    "        'Weather'\n",
    "    ]\n",
    "    \n",
    "    processed_data = {}\n",
    "    \n",
    "    for sensor in sensors:\n",
    "        file_path = os.path.join(base_path, f'{sensor.lower()}_sensor_dataset.csv')\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                df, label_encoders, scaler = process_dataset(\n",
    "                    file_path=file_path,\n",
    "                    sensor_name=sensor,\n",
    "                    target_column=None  # Specify if known\n",
    "                )\n",
    "                processed_data[sensor] = {\n",
    "                    'data': df,\n",
    "                    'label_encoders': label_encoders,\n",
    "                    'scaler': scaler\n",
    "                }\n",
    "                print(f\"Successfully processed {sensor} dataset\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sensor} dataset: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"Dataset not found for {sensor}\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "processed_data = process_all_sensors('path/to/data/directory')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
